{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    COMP4600/5500 - Reinforcement Learning\n",
    "\n",
    "# Homework 3 - Dynamic Programming\n",
    "\n",
    "### Due: Monday, October 4th 11:59 pm\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Mohamed Martini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to study different properties of dynamic programming methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are allowed to use the following modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {
    "g1313.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I\n",
    "Consider a cleaning robot that must collect an empty can and also has to recharge its batteries.\n",
    "![g1313.png](attachment:g1313.png)\n",
    "\n",
    "This problem has a discrete state space $S=\\{0,…,9\\}$, where state $s$ describes the position of the robot in the corridor. The robot has only two actions $A=\\{-1,1\\}$ for going one step to the left or right. States $0$ and $9$ are terminal, meaning that once the robot reaches either of them it can no longer leave, regardless of the action, and the episode ends. We assume this is a deterministic environment with $\\gamma=0.9$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \"\"\"\n",
    "    base class for a Q learner class with value iteration method\n",
    "    child classes shall only provide their custom dynamics\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 S,  # set of states\n",
    "                 A,  # set of actions\n",
    "                 gamma):  # discount parameter\n",
    "        self.S = S\n",
    "        self.num_states = S.shape[0]\n",
    "\n",
    "        self.A = A\n",
    "        self.num_actions = A.shape[0]\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.V = np.zeros(self.num_states)  # 9 states\n",
    "        self.Q = np.zeros((self.num_actions, self.num_states))  # 9 states and two actions per state\n",
    "\n",
    "    def dynamic_a(self, s):\n",
    "        \"\"\"return possible actions and their probabilities given a state s\"\"\"\n",
    "        return ([None], [None])\n",
    "    \n",
    "    def dynamic_s_pr(self, s, a):\n",
    "        \"\"\"return possible states and their probabilities given state s and action a\"\"\"\n",
    "        return ([None], [None])\n",
    "    \n",
    "    def dynamic_r(self, s, a, s_pr):\n",
    "        \"\"\"return possible rewards and their probabilities given state s and action a and next state s_pr\"\"\"\n",
    "        return ([None], [None])\n",
    "    \n",
    "    def v_iteration(self, thresh=0.01):\n",
    "        \"\"\"value iteration algorithm\"\"\"\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.num_states):\n",
    "#                 print(f\"{s =: }\")\n",
    "                v_s = 0\n",
    "                dynamic_a = self.dynamic_a(s)\n",
    "                for a, p_a in zip(dynamic_a[0], dynamic_a[1]):\n",
    "#                     print(f\"{a =: }, {p_a =: }\")\n",
    "                    q_as = 0\n",
    "                    dynamic_s_pr = self.dynamic_s_pr(s, a)\n",
    "                    for s_p, p_s_p in zip(dynamic_s_pr[0], dynamic_s_pr[1]):\n",
    "#                         print(f\"{s_p =: }, {p_s_p =: }\")\n",
    "                        dynamic_r = self.dynamic_r(s, a, s_p)\n",
    "                        for r, p_r in zip(dynamic_r[0], dynamic_r[1]):\n",
    "#                             print(f\"{r =: }, {p_r =: }\")\n",
    "                            q = p_s_p * p_r * (r + self.gamma * self.V[s_p])\n",
    "                            q_as += q\n",
    "                            v_s += p_a * q_as\n",
    "                            if q == np.inf:\n",
    "                                print(f\"self.V[{s_p}] = {self.V[s_p]}\")\n",
    "                                return\n",
    "#                             print(f\"s:\\t{s}\\ta:\\t{a}\\ts':\\t{a}\\tr:\\t{r}\")\n",
    "#                             print()\n",
    "#                             if input():\n",
    "#                                 return (0, 0)\n",
    "                            \n",
    "                    self.Q[a, s] = q_as\n",
    "                old = self.V[s]\n",
    "                self.V[s] = v_s\n",
    "                delta = max(delta, self.V[s] - old)\n",
    "            if delta < thresh:\n",
    "                break\n",
    "        return self.V, self.Q\n",
    "\n",
    "    def a_star(self):\n",
    "        \"\"\"get optimal policy\"\"\"\n",
    "        Pi_star = np.zeros(self.num_states, dtype=int)  # to hold optimal actions\n",
    "        for s in range(1, self.num_states - 1):\n",
    "            Pi_star[s] = self.A[np.argmax(self.Q[:, s])]\n",
    "        return Pi_star\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleaningRobot(QLearner):\n",
    "    \"\"\"Q learner class for problem 1\"\"\"\n",
    "    def __init__(self, \n",
    "                 S,  # set of states\n",
    "                 A,  # set of actions\n",
    "                 gamma):  # discount parameter\n",
    "        super(CleaningRobot, self).__init__(S, A, gamma)\n",
    "\n",
    "    def dynamic_a(self, s):\n",
    "        if s in (0, 9):\n",
    "            return ([], [])\n",
    "        \"\"\"get available actions in state s\"\"\"\n",
    "        return ([0, 1], [0.5, 0.5])\n",
    "    \n",
    "    def dynamic_s_pr(self, s, a):\n",
    "        \"\"\"state-transition function\"\"\"\n",
    "        if s in (0, 9):\n",
    "            return ([], [])\n",
    "        s_pr = min(max(s + self.A[a], 0), 9)\n",
    "        return ([s_pr], [1])\n",
    "    \n",
    "    def dynamic_r(self, s, a, s_pr):\n",
    "        \"\"\"reward function\"\"\"\n",
    "        if s == 8 and a == 1:\n",
    "            return ([5], [1])\n",
    "        if s == 1 and a == 0:\n",
    "            return ([1], [1])\n",
    "        return ([0], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_1d(array, name):\n",
    "    \"\"\"to print a 1d array\"\"\"\n",
    "    print(\"{} = (\".format(name))\n",
    "    [print(round(n,2), end=\",\\t\") for n in array]\n",
    "    print(\"\\n)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V* = (\n",
      "0.0,\t0.78,\t0.61,\t0.59,\t0.69,\t0.95,\t1.43,\t2.21,\t3.5,\t0.0,\t\n",
      ")\n",
      "\n",
      "π* = (\n",
      "0,\t-1,\t-1,\t1,\t1,\t1,\t1,\t1,\t1,\t0,\t\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "S = np.array(range(10))\n",
    "A = np.array((-1, 1), dtype=int)\n",
    "gamma = 0.9\n",
    "\n",
    "cr = CleaningRobot(S, A, gamma)\n",
    "V, Q = cr.v_iteration(thresh=0.001)\n",
    "Pi_star = cr.a_star()\n",
    "\n",
    "print_1d(V, \"V*\")\n",
    "print_1d(Pi_star, \"\\u03C0*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II (*)\n",
    "A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he has staked on that flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of \\$100, or loses by running out of money. \n",
    "\n",
    "On each flip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars. This problem can be formulated as an **undiscounted** ($\\gamma=1$), **episodic**, finite MDP.\n",
    "\n",
    "The state is the gambler’s capital $s \\in \\{ 1,2,...,99\\}$ and the actions are stakes $a \\in \\{ 0,1,..., min(s, 100-s)\\}$. The reward is zero on all transitions except those on which the gambler reaches his goal, when it is +1. \n",
    "The state-value function then gives the probability of winning from each state. \n",
    "\n",
    "A policy is mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal. \n",
    "Let’s p_h denote the probability of the coin coming up heads. If $p_h$ is known the problem can be solved using value iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1\n",
    "Implement the Gambler’s problem and then implement **value iteration** to solve the MDP for three scenarios where $p_h=\\{0.4,0.25,0.55\\}$ and find the optimal value function and optimal policy for each scenario.\n",
    "\n",
    "**Tip**: When implementing, you might find it convenient to introduce two dummy states corresponding to termination with capital of 0 and 100, giving them values of 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gambler(QLearner):\n",
    "    \"\"\"Q learner class for problem 1\"\"\"\n",
    "    def __init__(self, \n",
    "                 S,  # set of states\n",
    "                 A,  # set of actions\n",
    "                 gamma,  # discount parameter\n",
    "                 p_h):  # probability of head\n",
    "        super(Gambler, self).__init__(S, A, gamma)\n",
    "        self.p_h = p_h\n",
    "\n",
    "    def dynamic_a(self, s):\n",
    "        if s in (0, 100):\n",
    "            return ([], [])\n",
    "        \"\"\"get available actions in state s\"\"\"\n",
    "        num_actions = min(s, 100 - s)\n",
    "        actions = np.arange(1, num_actions + 1) / 1\n",
    "        probs = np.ones(num_actions) / num_actions  # equal probabilities\n",
    "        return (actions, probs)\n",
    "    \n",
    "    def dynamic_s_pr(self, s, a):\n",
    "        \"\"\"state-transition function\"\"\"\n",
    "        if s in (0, 100):\n",
    "            return ([], [])\n",
    "        s_pr_win = s + a  # agent wins\n",
    "        s_pr_lose = s - a  # agent loses\n",
    "        return ([s_pr_win , s_pr_lose], [self.p_h, 1 - self.p_h])\n",
    "    \n",
    "    def dynamic_r(self, s, a, s_pr):\n",
    "        \"\"\"reward function\"\"\"\n",
    "        if s_pr == 100:\n",
    "            return ([1.], [1.])\n",
    "        return ([0.], [1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-903b5b9c93a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprob2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGambler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mPi_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_star\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6dea0432e8c5>\u001b[0m in \u001b[0;36mv_iteration\u001b[0;34m(self, thresh)\u001b[0m\n\u001b[1;32m     48\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_r\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#                             print(f\"{r =: }, {p_r =: }\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_s_p\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp_r\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                             \u001b[0mq_as\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                             \u001b[0mv_s\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp_a\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "S = np.array(range(101), dtype=float)\n",
    "A = np.array(range(101), dtype=float)\n",
    "gamma = 1\n",
    "p_h = 0.4\n",
    "\n",
    "\n",
    "prob2 = Gambler(S, A, gamma, p_h)\n",
    "V, Q = prob2.v_iteration(thresh=0.01)\n",
    "Pi_star = prob2.a_star()\n",
    "\n",
    "print_1d(V, \"V*\")\n",
    "print_1d(Pi_star, \"\\u03C0*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2\n",
    "For all three scenarios:\n",
    "1. Plot the change in the value function over successive sweeps of value iteration w.r.t capital (state).\n",
    "2. Plot the final policy w.r.t capital (state). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3\n",
    "Answer the following questions:\n",
    "1. What action does your optimal policy suggest for capital of 50? What about for capital of 51?\n",
    "> Answer:\n",
    "\n",
    "2. Why do you think your optimal policy is a good policy? Explain.\n",
    "> Answer:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III (extra credit)\n",
    "Test the algorithm by decreasing $\\theta$ the threshold for accuracy of value function estimation. What happens when $\\theta \\rightarrow 0$? You can add any helpful code/graphs if you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
