{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    COMP4600/5300 - Reinforcement Learning\n",
    "\n",
    "# Homework 1 - K-armed Bandit Algorithms\n",
    "\n",
    "### Due: Monday, September 14th 11:59 pm\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: ______________________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this homework is to study different properties of multi-armed bandit algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are allowed to use the following modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 \n",
    "\n",
    "Build a testbed by generating 500 randomly selected k-armed bandit problems with $k = 7$.\n",
    "For each bandit problem, select the true action values from a Gaussian distribution with mean 0 and variance 1.0. For each action $a$, select an actual reward value from a normal distribution with mean $Q^*(a)$ and variance 1.0. For an algorithm, one run includes playing a single bandit problem for 1000 time steps. The algorithm's behavior will be evaluated by averaging its performance over 500 bandit problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that generates one k-armed bandit problem\n",
    "# that returns Q*(a) for that problem\n",
    "\n",
    "# Your code here\n",
    "def k_armed_bandit():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Implement the sample-average algorithm and run it on the testbed you developed in previous part according to the following settings:\n",
    "1. Using greedy action selection\n",
    "2. Using $\\varepsilon$-greedy action selection with $\\varepsilon=0.01$ and $\\varepsilon=0.1$\n",
    "3. Using upper-confidence bound action selection with $c=1$ and $c=2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that plays a given k-armed bandit problem\n",
    "# this function should include greedy, epsilon-greedy and UCB action selection strategies.\n",
    "# You can include all strategies in one function or write two functions (one for epsilon-greedy and another for UCB)\n",
    "# your implementation should return the selected action and the reward gained by selecting that action\n",
    "\n",
    "# Your code here\n",
    "def play_k_armed_bandit():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that plays all the games asked in the question\n",
    "# your implementation should loop over all strategies, all runs, and all time steps\n",
    "# and should output the collected rewards and number of best action selections for each strategy\n",
    "\n",
    "# Your code here\n",
    "\n",
    "def play_all():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previous function with actual values to run the experiments\n",
    "\n",
    "\n",
    "# Your code here\n",
    "num_arms = 7\n",
    "num_strategies = 5\n",
    "num_runs = 500\n",
    "num_timesteps = 1000\n",
    "\n",
    "play_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all three settings, plot the **average reward** and **%optimal action** graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the rewards and number of best action selections over all runs\n",
    "# then plot (a) average reward, (b) %optimal action over all time steps\n",
    "# you should plot two subplots (a and b) each showing all strategies\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. which action selection method performs worse than others? Why?\n",
    ">Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Which $\\varepsilon$ value improves faster? What is the best average reward value?\n",
    ">Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Which $\\varepsilon$ value will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be?\n",
    ">Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the difference between results from $c=1$ and $c=2$? Why?\n",
    ">Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Why is there a performance spike on the 8th step for the UCB method?\n",
    ">Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3*\n",
    "\n",
    "Implement the Gradient Bandit algorithm and plot the **average reward** and **%optimal action** graphs for the testbed you developed according to the following settings:\n",
    "1. Using $\\alpha=0.01$, $\\alpha=0.1$, and $\\alpha=0.5$\n",
    "2. Using no reward baseline, reward baseline of +5, reward baseline of +10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How do you compare the effect of reward baseline (discuss all scenarios)?\n",
    ">Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How do you compare the effect of step size (discuss all scenarios)?\n",
    ">Answer:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
