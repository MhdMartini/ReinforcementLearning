{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "technological-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pygame as pg\n",
    "from itertools import count, product\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920aba9",
   "metadata": {},
   "source": [
    "### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "educated-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pong table dimensions\"\"\"\n",
    "WIDTH = HEIGHT = 1\n",
    "\n",
    "\"\"\"pong peddles dimensions\"\"\"\n",
    "P_W = 0.2\n",
    "P_H = 0.02\n",
    "\n",
    "\"\"\"pong peddles y positions\"\"\"\n",
    "Y0 = 0.9\n",
    "Y1 = 0.1\n",
    "\n",
    "\"\"\"ball attributes\"\"\"\n",
    "BALL_R = 0.02\n",
    "BALL_VY = 1\n",
    "BALL_VX = 0\n",
    "\n",
    "\"\"\"state vector indices\"\"\"\n",
    "X0 = 0  # x position of peddle 0\n",
    "X1 = 1  # x position of peddle 1\n",
    "X_B = 2  # x position of ball\n",
    "Y_B = 3  # y position of ball\n",
    "VX_B = 4  # vx of ball\n",
    "VY_B = 5  # vy of ball\n",
    "\n",
    "MAX_V = 5\n",
    "dt = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f0aad",
   "metadata": {},
   "source": [
    "### Pong Transition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exterior-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pong_transition(s, a):\n",
    "    \"\"\"\n",
    "    given state and action vectors, return next state vector and reward\n",
    "    state vector is <x_{p0}, x_{p1}, x_{ball}, y_{ball}, v_x_{ball}, v_y_{ball}>\n",
    "    action_vector is <v_x_{p0}, v_x_{p1}}>\n",
    "    next state vector is <x_{p0} + v_x_{p0}dt, x_{p1} + v_x_{p1}dt, x_{ball} + v_x_{ball}dt, y_{ball} + v_y_{ball}dt, v_x_{ball}_{new}, v_y_{ball}_{new}>\n",
    "    \"\"\"\n",
    "\n",
    "    # get the peddles next positions\n",
    "    # if action takes peddle off the screen, effective action (peddle velocity) is 0\n",
    "    s_p = np.copy(s)\n",
    "    p_trans = s[: X_B] + a * dt * MAX_V\n",
    "    a[(p_trans < P_W / 2) | (p_trans > WIDTH - P_W / 2)] = 0\n",
    "    s_p[: X_B] += a * dt * MAX_V\n",
    "    \n",
    "    r = 0\n",
    "    win = 0\n",
    "    terminal = False\n",
    "    # if ball touches either peddle, reverse ball y velocity, and add peddle x velocity to ball x velocity\n",
    "    dy = s[VY_B] * dt\n",
    "    if s[Y_B] + dy <= Y1:\n",
    "        # if ball is as high as the top peddle\n",
    "        if abs(s[X_B] - s[X1]) <= P_W - 2 * BALL_R:\n",
    "            # if ball is on top peddle, \n",
    "            # flip y velocity, and add peddle x velocity to ball x velocity\n",
    "            s_p[VY_B] *= -1\n",
    "            s_p[VX_B] += a[1] * MAX_V\n",
    "        else:\n",
    "            # r = 5\n",
    "            terminal = True\n",
    "            win = 1\n",
    "            \n",
    "    elif s[Y_B] + dy >= Y0:\n",
    "        # if ball is as high as the top peddle\n",
    "        if abs(s[X_B] - s[X0]) <= P_W - 2 * BALL_R:\n",
    "            # if ball is on top peddle, \n",
    "            # flip y velocity, and add peddle x velocity to ball x velocity\n",
    "            s_p[VY_B] *= -1\n",
    "            s_p[VX_B] += a[0] * MAX_V\n",
    "            r = 1\n",
    "        else:\n",
    "            terminal = True\n",
    "            r = -1\n",
    "\n",
    "    # if ball touches sides, reverse ball x velocity\n",
    "    dx = s[VX_B] * dt\n",
    "    if s[X_B] + dx <= BALL_R or s[X_B] + dx >= 1 - BALL_R:\n",
    "        s_p[VX_B] *= -1\n",
    "        \n",
    "    # transition ball according to its velocity\n",
    "    s_p[X_B: VX_B] += s_p[VX_B: ] * dt\n",
    "\n",
    "\n",
    "    return s_p, r, terminal, win\n",
    "    \n",
    "    \n",
    "def pong_transition_solo(s, a):\n",
    "    \"\"\"\n",
    "    given state and action vectors, return next state vector and reward\n",
    "    state vector is <x_{p0}, x_{p1}, x_{ball}, y_{ball}, v_x_{ball}, v_y_{ball}>\n",
    "    action_vector is <v_x_{p0}, v_x_{p1}}>\n",
    "    next state vector is <x_{p0} + v_x_{p0}dt, x_{p1} + v_x_{p1}dt, x_{ball} + v_x_{ball}dt, y_{ball} + v_y_{ball}dt, v_x_{ball}_{new}, v_y_{ball}_{new}>\n",
    "    \"\"\"\n",
    "\n",
    "    # get the peddles next positions\n",
    "    # if action takes peddle off the screen, effective action (peddle velocity) is 0\n",
    "    s_p = np.copy(s)\n",
    "    p_trans = s[X0] + a[0] * dt * MAX_V\n",
    "    if not P_W / 2 < p_trans < WIDTH - P_W:\n",
    "        a[X0] = 0\n",
    "    s_p[X0] += a[0] * dt * MAX_V\n",
    "    \n",
    "    r = 0\n",
    "    terminal = False\n",
    "    # if ball touches either peddle, reverse ball y velocity, and add peddle x velocity to ball x velocity\n",
    "    dy = s[VY_B] * dt\n",
    "    if s[Y_B] + dy <= BALL_R:\n",
    "        # if ball is as high as the top \n",
    "        # flip y velocity, and add peddle x velocity to ball x velocity\n",
    "        s_p[VY_B] *= -1\n",
    "            \n",
    "    elif s[Y_B] + dy >= Y0:\n",
    "        # if ball is as high as the top peddle\n",
    "        if abs(s[X_B] - s[X0]) <= P_W - 2 * BALL_R:\n",
    "            # if ball is on top peddle, \n",
    "            # flip y velocity, and add peddle x velocity to ball x velocity\n",
    "            s_p[VY_B] *= -1\n",
    "            s_p[VX_B] += a[0] * MAX_V + np.random.uniform(-0.5, 0.5)\n",
    "            r = 1\n",
    "        else:\n",
    "            terminal = True\n",
    "            r = -1\n",
    "\n",
    "    # if ball touches sides, reverse ball x velocity\n",
    "    dx = s[VX_B] * dt\n",
    "    if s[X_B] + dx <= BALL_R or s[X_B] + dx >= 1 - BALL_R:\n",
    "        s_p[VX_B] *= -1\n",
    "        \n",
    "    # transition ball according to its velocity\n",
    "    s_p[X_B: VX_B] += s_p[VX_B: ] * dt\n",
    "\n",
    "    return s_p, r, terminal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d98369",
   "metadata": {},
   "source": [
    "### Pong Gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476863c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pong gui constants\"\"\"\n",
    "SCALE = 900\n",
    "PG_W, PG_H = WIDTH * SCALE, HEIGHT * SCALE\n",
    "PG_W_2, PG_H_2 = PG_W // 2, PG_H // 2\n",
    "PED_W, PED_H = int(P_W * SCALE), int(P_H * SCALE)\n",
    "PED_W2, PED_H2 = int(PED_W / 2), int(PED_H / 2)\n",
    "\n",
    "\n",
    "FPS = 30\n",
    "BG_COLOR = pg.Color(50, 50, 50)\n",
    "BORDER_COLOR = pg.Color(220, 220, 220)\n",
    "BALL_COLOR = pg.Color(200, 70, 70)\n",
    "PEDDLE_COLOR = pg.Color(240, 240, 240)\n",
    "\n",
    "class PongGui:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.screen, self.bg = self.init()\n",
    "        \n",
    "    def init(self):\n",
    "        pg.init()  # initialize pygame\n",
    "        screen = pg.display.set_mode((PG_W, PG_H))  # set up the screen\n",
    "        pg.display.set_caption(\"Mohamed Martini\")  # add a caption\n",
    "        bg = pg.Surface(screen.get_size())  # get a background surface\n",
    "        bg = bg.convert()\n",
    "        bg.fill(BG_COLOR)\n",
    "        screen.blit(bg, (0, 0))\n",
    "        return screen, bg\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"show the grid array on the screen\"\"\"\n",
    "        pg.display.flip()\n",
    "        pg.display.update()\n",
    "    \n",
    "    def draw_table(self):\n",
    "        pg.draw.line(self.screen, BORDER_COLOR, (0, PG_H_2), \n",
    "                     (PG_W, PG_H_2), width=2)\n",
    "        \n",
    "    \n",
    "    def draw_state(self, s):\n",
    "        for center_x, center_y in zip(s[X0: X0 + 2], [Y0, Y1]):\n",
    "            center_x = int(center_x * SCALE)\n",
    "            center_y = int(center_y * SCALE)\n",
    "            pg.draw.rect(self.screen, PEDDLE_COLOR, \n",
    "                         (center_x - PED_W2, \n",
    "                          center_y - PED_H2,\n",
    "                          PED_W,\n",
    "                          PED_H)\n",
    "                         )\n",
    "        \n",
    "        circle_center = s[X_B: VX_B] * SCALE\n",
    "        pg.draw.circle(self.screen, BALL_COLOR, \n",
    "                       circle_center.astype(int),\n",
    "                       int(BALL_R * SCALE), \n",
    "                       width=int(BALL_R * SCALE))\n",
    "    \n",
    "    def reset_screen(self):\n",
    "        self.screen.fill(BG_COLOR)\n",
    "        self.draw_table()\n",
    "    \n",
    "    def play(self, theta0=None, theta1=None):\n",
    "        \"\"\"receive a list of positions on the x axis, and plot the movement of the screen\"\"\"\n",
    "        if theta0 is None:\n",
    "            theta0 = np.zeros(NUM_FEATURES * NUM_ACTIONS)\n",
    "        if theta1 is None:\n",
    "            theta1 = np.zeros(NUM_FEATURES * NUM_ACTIONS)\n",
    "\n",
    "        s_sim = get_s0_sim()\n",
    "        s = transform(s_sim, direction=1)\n",
    "        score = 0\n",
    "        \n",
    "        clock = pg.time.Clock()\n",
    "        run = True\n",
    "        while run:\n",
    "            clock.tick(FPS)\n",
    "            for event in pg.event.get():\n",
    "                if event.type == pg.QUIT:\n",
    "                    run = False\n",
    "            self.reset_screen()\n",
    "            self.draw_table()\n",
    "            self.draw_state(s)\n",
    "\n",
    "            # take action, observe reward and next state\n",
    "            a0, policy0 = get_action(s[KI], theta0)\n",
    "            a1 = np.random.choice(NUM_ACTIONS)\n",
    "            a_sim = np.array([A[a0], A[a1]])\n",
    "\n",
    "#             sp_sim, r, terminal, win = pong_transition(s_sim, a_sim)\n",
    "            sp_sim, r, terminal = pong_transition_solo(s_sim, a_sim)\n",
    "            sp = transform(sp_sim, direction=1)\n",
    "\n",
    "            s = sp\n",
    "            s_sim = sp_sim\n",
    "            score += r\n",
    "            \n",
    "            # print score\n",
    "            myFont = pg.font.SysFont(\"Times New Roman\", 32)\n",
    "            score_disp = myFont.render(str(int(score)), 1, BALL_COLOR)\n",
    "            self.screen.blit(score_disp, (10, 10))\n",
    "\n",
    "            self.render()\n",
    "            \n",
    "            if terminal:\n",
    "                s_sim = get_s0_sim()\n",
    "                s = transform(s_sim, direction=1)\n",
    "\n",
    "        pg.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9093906",
   "metadata": {},
   "source": [
    "### Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6baf2",
   "metadata": {},
   "source": [
    "#### Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e8b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_s(s: np.array):\n",
    "    \"\"\"return x(s) as fourier basis of state\"\"\"\n",
    "    x = np.zeros(NUM_FEATURES)\n",
    "    for i, c in enumerate(product(range(D + 1), repeat=K)):\n",
    "        c = np.array(c)\n",
    "        x[i] = np.cos(np.pi * s.T @ c)\n",
    "    return x\n",
    "\n",
    "\n",
    "def x_sa(s: np.array, a: int):\n",
    "    \"\"\"return x(s, a) as fourier basis of state, shifted according to the action index\"\"\"\n",
    "    x = np.zeros(NUM_FEATURES * NUM_ACTIONS)\n",
    "    start = NUM_FEATURES * a\n",
    "    end = start + NUM_FEATURES\n",
    "    x[start: end] = x_s(s)\n",
    "    return x\n",
    "\n",
    "\n",
    "def h_s(s: np.array, theta: np.array):\n",
    "    \"\"\"return actions' preferences in state s\"\"\"\n",
    "    h = np.zeros(NUM_ACTIONS)\n",
    "    for a in range(NUM_ACTIONS):\n",
    "        h[a] = theta @ x_sa(s, a)\n",
    "    return h\n",
    "\n",
    "\n",
    "def pi_s(s: np.array, theta: np.array):\n",
    "    \"\"\"return policy at state s\"\"\"\n",
    "    h = h_s(s, theta)\n",
    "    exp = np.exp(h - np.max(h))\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "\n",
    "def v_s(s: np.array, w: np.array):\n",
    "    \"\"\"return the value of a state given the weights vector\"\"\"\n",
    "    return w @ x_s(s)\n",
    "\n",
    "\n",
    "def get_action(s, theta):\n",
    "    \"\"\"return index of action at state s according to weights theta\"\"\"\n",
    "    policy = pi_s(s, theta)\n",
    "    return np.random.choice(range(NUM_ACTIONS), p=policy), policy\n",
    "\n",
    "\n",
    "def get_pi_gradient(s, a, policy):\n",
    "    \"\"\"compute gradient ln pi(a|s, theta), which equals x(s,a) = \\sum_b \\pi(b|s, theta) x(s,b)\"\"\"\n",
    "    x = x_sa(s, a)\n",
    "    summation = 0\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        summation += policy[i] * x_sa(s, i)\n",
    "    return x - summation\n",
    "\n",
    "\n",
    "def get_s0_sim():\n",
    "    s = np.random.uniform(0.11, 0.9, size=K_sim)\n",
    "    s[Y_B] = 0.5\n",
    "    direction = np.random.choice((-1, 1))\n",
    "    s[VX_B] = 0\n",
    "    s[VY_B] = direction * MAX_V\n",
    "    return s\n",
    "\n",
    "# def get_s0_sim():\n",
    "#     s = np.zeros(K_sim)\n",
    "#     s[X0: VX_B] = 0.5\n",
    "#     direction = np.random.choice((-1, 1))\n",
    "#     s[VX_B] = direction * 0.35 * MAX_V #np.random.uniform(0.45, 0.55)\n",
    "#     s[VY_B] = direction * 0.65 * MAX_V\n",
    "#     return s\n",
    "\n",
    "def transform(s, direction=1):\n",
    "    # direction = 0 -> s to s_sim. \n",
    "    # direction = 1 -> s_sim to s. \n",
    "    dirs = ((0, 1), (-MAX_V, MAX_V))\n",
    "    _s = np.copy(s)\n",
    "    _s[VX_B: ] = np.interp(s[VX_B: ], dirs[direction], dirs[1 - direction])\n",
    "    return _s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac9109",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea92672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_et(theta0, theta1, num_episodes):\n",
    "    gamma = 0.99\n",
    "    \n",
    "    W = np.zeros(NUM_FEATURES)  # weights for estimating v_s\n",
    "    \n",
    "    lambda_w = 0.5\n",
    "    lambda_theta = 0.5\n",
    "    \n",
    "    alpha_w = 1e-4\n",
    "    alpha_theta = 1e-5\n",
    "    \n",
    "    steps_per_e = np.zeros(num_episodes)\n",
    "    rewards = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # initialize s - simulation state is not normalized\n",
    "        s_sim = get_s0_sim()\n",
    "        s = transform(s_sim, direction=1)\n",
    "        s = np.array([s[i] for i in range(s.shape[0]) if i != 1])\n",
    "\n",
    "        # reset z vectors\n",
    "        z_theta = np.zeros_like(theta0)\n",
    "        z_w = np.zeros_like(W)\n",
    "\n",
    "        # reset gamma multiplier\n",
    "        I = 1\n",
    "        \n",
    "        # loop through episode\n",
    "        for t in count():\n",
    "            # select action\n",
    "            a0, policy0 = get_action(s, theta0)\n",
    "            a1 = np.random.choice(NUM_ACTIONS)\n",
    "            # if t == 0 and not episode % 100:\n",
    "            #     d = \"right\" if s[VX_B] > 0 else \"left\"\n",
    "            #     print(f\"{d.upper()} Initial Policy\", policy0)\n",
    "            \n",
    "            # take action, observe reward and next state\n",
    "            a_sim = np.array([A[a0], A[a1]])\n",
    "#             s_p_sim, r, terminal, win = pong_transition(s_sim, a_sim)\n",
    "            s_p_sim, r, terminal = pong_transition_solo(s_sim, a_sim)\n",
    "            rewards += r\n",
    "            s_p = transform(s_p_sim, direction=1)\n",
    "            s_p = np.array([s_p[i] for i in range(s_p.shape[0]) if i != 1])\n",
    "            \n",
    "            # calculate the error (delta) - account for terminal state\n",
    "            if terminal:\n",
    "                v_sp = 0\n",
    "            else:\n",
    "                v_sp = v_s(s_p, W)\n",
    "            \n",
    "            delta = r + gamma * v_sp  - v_s(s, W)\n",
    "            # if t == 0:\n",
    "            #     print(round(delta, 2))\n",
    "            \n",
    "            # update z_w\n",
    "            z_w = gamma * lambda_w * z_w + x_s(s)\n",
    "            \n",
    "            # update z_theta\n",
    "            gradient = get_pi_gradient(s, a0, policy0)\n",
    "            z_theta = gamma * lambda_theta * z_theta + I * gradient\n",
    "            \n",
    "            # update w\n",
    "            W += alpha_w * delta * z_w\n",
    "            \n",
    "            # update theta\n",
    "            theta0 += alpha_theta * delta * z_theta\n",
    "            \n",
    "            if terminal or t > 1_000:\n",
    "                break\n",
    "            \n",
    "            I *= gamma\n",
    "            s = s_p\n",
    "            s_sim = s_p_sim\n",
    "        \n",
    "        steps_per_e[episode] = t\n",
    "            \n",
    "    return theta0, theta1, steps_per_e, rewards\n",
    "\n",
    "def play_offline(theta0, theta1, num_episodes):\n",
    "\n",
    "    steps_per_e = np.zeros(num_episodes)\n",
    "    rewards = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # initialize s - simulation state is not normalized\n",
    "        s_sim = get_s0_sim()\n",
    "        s = transform(s_sim, direction=1)\n",
    "        \n",
    "        # loop through episode\n",
    "        for t in count():\n",
    "            # select action\n",
    "            a0, policy0 = get_action(s[KI], theta0)\n",
    "            a1 = np.random.choice(NUM_ACTIONS)\n",
    "            \n",
    "            # take action, observe reward and next state\n",
    "            a_sim = np.array([A[a0], A[a1]])\n",
    "#             s_p_sim, r, terminal, win = pong_transition(s_sim, a_sim)\n",
    "            s_p_sim, r, terminal = pong_transition_solo(s_sim, a_sim)\n",
    "            s_p = transform(s_p_sim, direction=1)\n",
    "\n",
    "            rewards += r          \n",
    "            if terminal or t > 1_000:\n",
    "                break\n",
    "            \n",
    "            s = s_p\n",
    "            s_sim = s_p_sim\n",
    "        \n",
    "        steps_per_e[episode] = t\n",
    "            \n",
    "    return steps_per_e, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723f8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Weights ...\n"
     ]
    }
   ],
   "source": [
    "A = np.array([-1, 0, 1])\n",
    "NUM_ACTIONS = 3\n",
    "K = 5\n",
    "K_sim = 6\n",
    "KI = [i for i in range(K_sim) if i != 1]\n",
    "D = 3\n",
    "NUM_FEATURES = (D + 1) ** K\n",
    "\n",
    "batch = 100\n",
    "batches = 100\n",
    "num_episodes = batch * batches\n",
    "\n",
    "base_path = \"colab/solo\"\n",
    "filename = os.path.join(base_path, \"BEST_WEIGHTS_815\")\n",
    "# base_name, idx = filename.split(\"_\")\n",
    "try:\n",
    "    theta0 = np.load(f\"{filename}.npy\")\n",
    "    print(\"Importing Weights ...\")\n",
    "except:\n",
    "    theta0 = np.zeros(NUM_ACTIONS * NUM_FEATURES)  # theta for each action\n",
    "theta1 = np.zeros_like(theta0)\n",
    "\n",
    "# test_batch = 100\n",
    "# for i in tqdm(range(1, batches + 1)):\n",
    "#     theta0, theta1, steps_per_e = actor_critic_et(theta0, theta1, batch)\n",
    "#     steps_per_e, win_perc = play_offline(theta0, theta1, test_batch)\n",
    "#     win_perc = win_perc / test_batch * 100\n",
    "#     print(f\"win perc. {win_perc}%\")\n",
    "#     print(f\"steps_per_e mean: {steps_per_e.mean()}\")\n",
    "#     outfile = f\"{base_name}_{i + int(idx)}\"\n",
    "#     outfile = os.path.join(base_path, outfile)\n",
    "#     np.save(outfile, theta0)\n",
    "#     print(f\"new weights saved as {outfile}.npy\")\n",
    "\n",
    "#     if win_perc > best_weights:\n",
    "#         best_weights = win_perc\n",
    "#         np.save(os.path.join(base_path, \"BEST_WEIGHTS\"), theta0)\n",
    "#         print(f\"Best weights {best_weights} saved!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9504f73a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PongGui().play(theta0, theta1)\n",
    "# PongGui().play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dress-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = \"colab\"\n",
    "# num_episodes = 100\n",
    "\n",
    "# y = []\n",
    "# x = []\n",
    "# for filename in tqdm(os.listdir(base_path)):\n",
    "#     if not filename.endswith(\".npy\"):\n",
    "#         continue\n",
    "#     basename, fmt = filename.split(\".\")\n",
    "#     index = int(basename.split(\"_\")[-1])\n",
    "#     if index > 100:\n",
    "#         continue\n",
    "\n",
    "#     x.append(index*chunk)\n",
    "#     filename = os.path.join(base_path, filename)\n",
    "#     theta0 = np.load(filename)\n",
    "#     score = PongGui.play_offline(theta0=theta0, num_episodes=num_episodes)\n",
    "#     y.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "burning-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(x, y)\n",
    "# plt.axhline(num_episodes)\n",
    "# plt.axhline(-num_episodes, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-pasta",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
