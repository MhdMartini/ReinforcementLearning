{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informative-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "%run env_test.ipynb\n",
    "%run agent.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apart-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_episodes, env, agent, vis=False):\n",
    "    steps_per_e = np.zeros(num_episodes)\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        # choose s0\n",
    "        s = env.reset()\n",
    "        agent.store_s(s, t=0)\n",
    "        \n",
    "        # choose a0\n",
    "        a = agent.choose_action(s)\n",
    "        agent.store_a(s, a, t=0)\n",
    "        \n",
    "        # T <- inf\n",
    "        T = np.inf\n",
    "        for t in count():\n",
    "            if t >= 100_000:\n",
    "                print(\"too long\")\n",
    "                break\n",
    "            if t < T:\n",
    "                # take action a(t)\n",
    "                a = agent.a_t[t % (agent.n + 1)]\n",
    "\n",
    "                # observe and store r and sp\n",
    "                sp, r, terminal = env.step(a)\n",
    "                agent.store_s(sp, t+1)\n",
    "                agent.store_r(r, t+1)\n",
    "\n",
    "                if terminal:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # select and store a(t+1)\n",
    "                    ap = agent.choose_action(sp)\n",
    "                    agent.store_a(sp, ap, t=t + 1)\n",
    "            \n",
    "            tau = t - agent.n + 1\n",
    "            if tau >= 0:\n",
    "                agent.learn(tau, T)\n",
    "                \n",
    "            if tau == T - 1:\n",
    "                break\n",
    "\n",
    "            if vis:\n",
    "                cont = env.render()\n",
    "                if not cont:\n",
    "                    pg.quit()\n",
    "                    vis = False\n",
    "\n",
    "        steps_per_e[episode] = t\n",
    "        r_per_e[episode] = r_t\n",
    "\n",
    "    try:\n",
    "        pg.quit()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return Q, steps_per_e\n",
    "\n",
    "def train(num_episodes, env, agent, vis=False):\n",
    "    steps_per_e = np.zeros(num_episodes)\n",
    "    r_per_e = np.zeros(num_episodes)\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        terminal = False\n",
    "        s = env.reset()\n",
    "        r_t = 0\n",
    "        t = 0\n",
    "        while not terminal:\n",
    "            t += 1\n",
    "            a = agent.choose_action(s)\n",
    "            sp, r, terminal = env.step(a)\n",
    "            Q, s_count, sa_count = agent.learn(s, a, r, sp)\n",
    "            r_t += r\n",
    "            s = sp\n",
    "\n",
    "            if vis:\n",
    "                cont = env.render()\n",
    "                if not cont:\n",
    "                    pg.quit()\n",
    "                    vis = False\n",
    "\n",
    "        steps_per_e[episode] = t\n",
    "        r_per_e[episode] = r_t\n",
    "\n",
    "    try:\n",
    "        pg.quit()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return Q, s_count, sa_count, steps_per_e, r_per_e\n",
    "\n",
    "def play(env, Q):\n",
    "    pg.init()\n",
    "    agent = AgentSmall(n_actions, n_dims, gamma, alpha_p, Q)\n",
    "    terminal = False\n",
    "    s = env.reset()\n",
    "    while True:\n",
    "        a = agent.choose_action_egreedy(s)\n",
    "        sp, r, terminal = env.step(a)\n",
    "        if terminal:\n",
    "            pg.quit()\n",
    "            return\n",
    "        cont = env.render()\n",
    "        if cont is False:\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "oriented-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    actions = np.array([[0, 1], [-1, 0], [0, -1], [1, 0]], dtype=int)\n",
    "    n_actions = actions.shape[0]\n",
    "    n_dims = (n_actions, H, W)\n",
    "    gamma = 0.99\n",
    "    alpha_p = 0.99\n",
    "    \n",
    "    n_states = 2\n",
    "    \n",
    "    env = TestEnv(n_dims, n_states, actions)\n",
    "    num_episodes = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "brown-missile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-696347622540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  alpha_p=alpha_p)    \n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Q_s_{num_episodes + offset}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-207eaedf76c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, env, agent, vis)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e5b28ea59f94>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;34m\"\"\"get action with e-greedy policy\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# update s_count()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# choose e-greedy action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "agent = Agent( \n",
    "                 n_actions=n_actions, \n",
    "                 n_dims=n_dims, \n",
    "                 gamma=gamma, \n",
    "                 alpha_p=alpha_p)    \n",
    "Q, steps_per_e = train(num_episodes=num_episodes, env=env, agent=agent, vis=True)\n",
    "\n",
    "np.save(f\"Q_s_{num_episodes + offset}\", Q)\n",
    "np.save(f\"s_count_s_{num_episodes + offset}\", s_count)\n",
    "np.save(f\"sa_count_s_{num_episodes + offset}\", sa_count)\n",
    "\n",
    "plt.scatter(range(num_episodes), steps_per_e)\n",
    "plt.scatter(range(num_episodes), r_per_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-corps",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
