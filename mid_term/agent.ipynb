{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "textile-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-revision",
   "metadata": {},
   "source": [
    "state space:\n",
    "$$\n",
    "{x_p \\in \\mathcal{R}^N, y_p \\in \\mathcal{R}^N, x_{h} \\in \\mathcal{R}^N, button \\in [0, 1]}\n",
    "$$\n",
    "action space:\n",
    "$$\n",
    "{direction \\in {up, down, left, right}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q \\in \\mathcal{R}^{4 \\times N \\times N \\times N \\times 2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "delayed-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_argmax(array):\n",
    "    \"\"\"return argmax and break ties\"\"\"\n",
    "    max_ = np.nanmax(array)\n",
    "    indx = [i for i in range(len(array)) if array[i] == max_]\n",
    "    return np.random.choice(indx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "known-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, n_actions, n_dims, gamma, alpha_p=1, \n",
    "                 Q=None, s_count=None, sa_count=None):\n",
    "        self.n_actions = n_actions\n",
    "        self.action_space = range(n_actions)\n",
    "        self.n_dims = n_dims\n",
    "        self.gamma = gamma\n",
    "        self.alpha_p = alpha_p\n",
    "        \n",
    "        self.s_count = np.zeros(n_dims[1:]) if s_count is None else s_count\n",
    "        self.sa_count = np.zeros(n_dims) if sa_count is None else sa_count\n",
    "        self.Q = np.zeros(n_dims) if Q is None else Q\n",
    "        \n",
    "    def learn(self, s, a, r, sp):\n",
    "        \"\"\"update Q_as according to observation and learning rate\"\"\"\n",
    "        # update Q according to error\n",
    "        self.sa_count[a, (*s)] += 1\n",
    "        error = r + self.gamma * (np.max(self.Q[self.action_space, sp[0], sp[1], sp[2], sp[3]])) - self.Q[a,  s[0], s[1], s[2], s[3]]\n",
    "        alpha = 1 / self.sa_count[a,  s[0], s[1], s[2], s[3]] ** self.alpha_p\n",
    "        self.Q[a, s[0], s[1], s[2], s[3]] += alpha * error \n",
    "        \n",
    "        return self.Q, self.s_count, self.sa_count\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        \"\"\"get action with e-greedy policy\"\"\"\n",
    "        # update s_count()\n",
    "        self.s_count[s[0], s[1], s[2], s[3]] += 1\n",
    "\n",
    "        # choose e-greedy action \n",
    "        eps = 1 / np.sqrt(self.s_count[ s[0], s[1], s[2], s[3]])\n",
    "        if np.random.uniform() < eps:\n",
    "            return np.random.choice(self.n_actions)\n",
    "        return my_argmax(self.Q[self.action_space,  s[0], s[1], s[2], s[3]])\n",
    "    \n",
    "    def choose_action_egreedy(self, s, eps=0.1):\n",
    "        \"\"\"get action with e-greedy policy\"\"\"\n",
    "        if np.random.uniform() < eps:\n",
    "            return np.random.choice(self.n_actions)\n",
    "        return my_argmax(self.Q[self.action_space,  s[0], s[1], s[2], s[3]])\n",
    "\n",
    "    def reset(self):\n",
    "        self.Q = np.zeros(n_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentSmall:\n",
    "    def __init__(self, n_actions, n_dims, gamma, alpha_p=1, \n",
    "                 Q=None, s_count=None, sa_count=None):\n",
    "        self.n_actions = n_actions\n",
    "        self.action_space = range(n_actions)\n",
    "        self.n_dims = n_dims\n",
    "        self.gamma = gamma\n",
    "        self.alpha_p = alpha_p\n",
    "        \n",
    "        self.s_count = np.zeros(n_dims[1:]) if s_count is None else s_count\n",
    "        self.sa_count = np.zeros(n_dims) if sa_count is None else sa_count\n",
    "        self.Q = np.zeros(n_dims) if Q is None else Q\n",
    "        \n",
    "    def learn(self, s, a, r, sp):\n",
    "        \"\"\"update Q_as according to observation and learning rate\"\"\"\n",
    "        # update Q according to error\n",
    "        self.sa_count[a, s[0], s[1], s[2]] += 1\n",
    "        error = r + self.gamma * (np.max(self.Q[self.action_space, sp[0], sp[1], sp[2]])) - self.Q[a, s[0], s[1], s[2]]\n",
    "        alpha = 1 / self.sa_count[a, s[0], s[1], s[2]] ** self.alpha_p\n",
    "        self.Q[a, s[0], s[1], s[2]] += alpha * error \n",
    "        \n",
    "        return self.Q, self.s_count, self.sa_count\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        \"\"\"get action with e-greedy policy\"\"\"\n",
    "        # update s_count()\n",
    "        self.s_count[s[0], s[1], s[2]] += 1\n",
    "\n",
    "        # choose e-greedy action \n",
    "        eps = 1 / np.sqrt(self.s_count[s[0], s[1], s[2]])\n",
    "        if np.random.uniform() < eps:\n",
    "            return np.random.choice(self.n_actions)\n",
    "        return my_argmax(self.Q[self.action_space, s[0], s[1], s[2]])\n",
    "    \n",
    "    def choose_action_egreedy(self, s, eps=0.1):\n",
    "        \"\"\"get action with e-greedy policy\"\"\"\n",
    "        if np.random.uniform() < eps:\n",
    "            return np.random.choice(self.n_actions)\n",
    "        return my_argmax(self.Q[self.action_space, s[0], s[1], s[2]])\n",
    "\n",
    "    def reset(self):\n",
    "        self.Q = np.zeros(n_dims)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
