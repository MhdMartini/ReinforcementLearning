{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are allowed to use the following modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mountain_car import MountainCar\n",
    "import pygame as pg\n",
    "from itertools import product, count\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\"\"\"\n",
    "Comment if you get an error, or install the following library to use latex with matplotlib on Linux:\n",
    "sudo apt-get install texlive-latex-extra texlive-fonts-recommended dvipng cm-super\n",
    "\"\"\"\n",
    "plt.rcParams['text.usetex'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _x_s(s: np.array):\n",
    "    \"\"\"return x(s) as fourier basis of state\"\"\"\n",
    "    x = np.zeros(NUM_FEATURES)\n",
    "    for i, c in enumerate(product(range(D + 1), repeat=K)):\n",
    "        c = np.array(c)\n",
    "        x[i] = np.cos(np.pi * s.T @ c)\n",
    "    return x\n",
    "\n",
    "\n",
    "def x_s(s: np.array):\n",
    "    \"\"\"\n",
    "    return x(s) as fourier basis of state. \n",
    "    Look first if previously computated, else compute and store it\n",
    "    \"\"\"\n",
    "    global BASIS\n",
    "    try:\n",
    "        return BASIS[tuple(s)]\n",
    "    except KeyError:\n",
    "        BASIS[tuple(s)] = _x_s(s)\n",
    "        return BASIS[tuple(s)]\n",
    "\n",
    "def x_sa(s: np.array, a: int):\n",
    "    \"\"\"return x(s, a) as fourier basis of state, shifted according to the action index\"\"\"\n",
    "    x = np.zeros(NUM_FEATURES * NUM_ACTIONS)\n",
    "    start = NUM_FEATURES * a\n",
    "    end = start + NUM_FEATURES\n",
    "    x[start: end] = x_s(s)\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def h_s(s: np.array, theta: np.array):\n",
    "    \"\"\"return actions' preferences in state s\"\"\"\n",
    "    h = np.zeros(NUM_ACTIONS)\n",
    "    for a in range(NUM_ACTIONS):\n",
    "        h[a] = theta @ x_sa(s, a)\n",
    "    return h\n",
    "\n",
    "def pi_s(s: np.array, theta: np.array):\n",
    "    \"\"\"return policy at state s\"\"\"\n",
    "    h = h_s(s, theta)\n",
    "    exp = np.exp(h - np.max(h))\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "def v_s(s: np.array, w: np.array):\n",
    "    \"\"\"return the value of a state given the weights vector\"\"\"\n",
    "    return w @ x_s(s)\n",
    "\n",
    "def get_action(s, theta):\n",
    "    \"\"\"return index of action at state s according to weights theta\"\"\"\n",
    "    policy = pi_s(s, theta)\n",
    "    return np.random.choice(range(NUM_ACTIONS), p=policy), policy\n",
    "\n",
    "def get_pi_gradient(s, a, policy):\n",
    "    \"\"\"compute gradient ln pi(a|s, theta), which equals x(s,a) = \\sum_b \\pi(b|s, theta) x(s,b)\"\"\"\n",
    "    x = x_sa(s, a)\n",
    "    summation = 0\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        summation += policy[i] * x_sa(s, i)\n",
    "    return x - summation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_et(num_episodes):\n",
    "    player1 = PongPlayer()\n",
    "    gamma = 1\n",
    "    theta = np.zeros(NUM_ACTIONS * NUM_FEATURES)  # theta for each action\n",
    "    W = np.zeros(NUM_FEATURES)  # weights for estimating v_s\n",
    "    \n",
    "    lambda_w = 0.8\n",
    "    lambda_theta = 0.8\n",
    "    \n",
    "    alpha_w = 1e-3\n",
    "    alpha_theta = 1e-3\n",
    "    \n",
    "    steps_per_e = np.zeros(num_episodes)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # initialize s\n",
    "        xv = np.array((np.random.uniform(-0.6, -0.4), 0))\n",
    "        s = xv_to_s(xv)\n",
    "\n",
    "        # reset z vectors\n",
    "        z_theta = np.zeros_like(theta)\n",
    "        z_w = np.zeros_like(W)\n",
    "\n",
    "        # reset gamma multiplier\n",
    "        I = 1\n",
    "        \n",
    "        # reset trajectory\n",
    "        traj = [xv[0]]\n",
    "        \n",
    "        # loop through episode\n",
    "        for t in count():\n",
    "            # select action\n",
    "            a, policy = get_action(s, theta)\n",
    "            \n",
    "            # take action, observe reward and next state\n",
    "            x, v = s_to_xv(s)\n",
    "            xp, vp, r, goal_reached = car.move(x, v, A[a])\n",
    "            sp = xv_to_s(np.array((xp, vp)))\n",
    "            traj.append(xp)\n",
    "            \n",
    "            # calculate the error (delta) - account for terminal state\n",
    "            if goal_reached:\n",
    "                v_sp = 0\n",
    "            else:\n",
    "                v_sp = v_s(sp, W)\n",
    "            delta = r + gamma * v_sp  - v_s(s, W)\n",
    "            \n",
    "            # update z_w\n",
    "            z_w = gamma * lambda_w * z_w + x_s(s)\n",
    "            \n",
    "            # update z_theta\n",
    "            gradient = get_pi_gradient(s, a, policy)\n",
    "            z_theta = gamma * lambda_theta * z_theta + I * gradient\n",
    "            \n",
    "            # update w\n",
    "            W += alpha_w * delta * z_w\n",
    "            \n",
    "            # update theta\n",
    "            theta += alpha_theta * delta * z_theta\n",
    "            \n",
    "            if goal_reached:\n",
    "                car.goal_reached = False\n",
    "                break\n",
    "            \n",
    "            I *= gamma\n",
    "            s = sp\n",
    "        \n",
    "        steps_per_e[episode] = t\n",
    "            \n",
    "    return theta, traj, steps_per_e\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
