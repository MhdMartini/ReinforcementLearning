{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0eY5jMMlJHM"
   },
   "source": [
    "<center>\n",
    "    COMP4600/5500 - Reinforcement Learning\n",
    "\n",
    "# Homework 8 - Policy Gradient\n",
    "\n",
    "### Due: Monday, November 29th 11:59 pm\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H6mIZCWlJHS"
   },
   "source": [
    "Student Name: ______________________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhd9RUL9lJHT"
   },
   "source": [
    "The purpose of this project is to study different properties of Policy Gradient algorithms with Function Approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LDVhFg1YlJHU"
   },
   "outputs": [],
   "source": [
    "# You are allowed to use the following modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4dNBKnolJHW"
   },
   "source": [
    "## Task description\n",
    "Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram in the upper left of the following figure. The difficulty is that gravity is stronger than the car's engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to first move away from the goal and up the opposite slope on the left. Then by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way.\n",
    "\n",
    "\n",
    "![mc.png](attachment:mc.png)\n",
    "\n",
    "\n",
    "This is a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. The reward in this problem is -1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (-1), and zero throttle (0). The car moves according to a simplified physics. Its position $x_t$ and velocity $\\dot{x}_t$ are updated by\n",
    "\n",
    "$x_{t+1} \\doteq \\text{bound}[x_t + \\dot{x}_{t+1}]$\n",
    "\n",
    "$\\dot{x}_{t+1} \\doteq \\text{bound}[\\dot{x}_t + 0.001 A_t - 0.0025 \\cos(3x_t)]$\n",
    "\n",
    "\n",
    "where the \\textit{bound} operation enforces $-1.2 \\le x_{t+1} \\le 0.5$ and $-0.07 \\le \\dot{x}_{t+1} \\le 0.07$. In addition, when $x_{t+1}$ reached the left bound, $\\dot{x}_{t+1}$ was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode starts from a random position $x_t \\in [-0.6, -0.4)$ and zero velocity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Note:** You have been given a simple implementation of the Mountain Car task. You can use your implementation of the function approximation from Homework 6, or implement a new one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GInvsnBFlJHX"
   },
   "source": [
    "## Part I (COMP4600)\n",
    "\n",
    "Implement REINFORCE with Baseline (p. 330).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zxhpY6MflJHY"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations, product\n",
    "import numpy as np\n",
    "def _build_coefficients(order,state_dim,max_non_zero):\n",
    "    coeff = np.array(np.zeros(state_dim))\n",
    "\n",
    "    for i in range(1,max_non_zero + 1):\n",
    "        for indices in combinations(range(state_dim), i):\n",
    "            for c in product(range(1, order + 1), repeat=i):\n",
    "                coef = np.zeros(state_dim)\n",
    "                coef[list(indices)] = list(c)\n",
    "                coeff = np.vstack((coeff, coef))\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GRevCTMwlJHZ"
   },
   "outputs": [],
   "source": [
    "coeff = _build_coefficients(3,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apAVtgdplJHa",
    "outputId": "d7c7e882-d66d-4cc5-f84e-4352e503cab5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i1Dzn4lVlJHb"
   },
   "outputs": [],
   "source": [
    "def Xi(state,c):\n",
    "    p=convert_position(state[0])\n",
    "    v=convert_velocity(state[1])\n",
    "    state=(p,v)\n",
    "    t=np.dot(np.pi*c, np.array(state).reshape(2,1))\n",
    "    return np.cos(t) #.reshape(9,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "myM-On2SBqIr"
   },
   "outputs": [],
   "source": [
    "new_features={}\n",
    "actions = [-1, 0 , 1]\n",
    "for a in actions:\n",
    "    new_features[a]=np.zeros(48).reshape(48,1)\n",
    "pad=np.zeros((16,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mWRXl0EXBB3J"
   },
   "outputs": [],
   "source": [
    "def new_Xi(position,velocity):\n",
    "  global new_features,pad,coeff\n",
    "  feature=Xi((position,velocity),coeff)\n",
    "  new_features[-1]=np.concatenate((feature,pad,pad))\n",
    "  new_features[0]=np.concatenate((pad,feature,pad))\n",
    "  new_features[1]=np.concatenate((pad,pad,feature))\n",
    "  return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "B3IVpCa2lJHc"
   },
   "outputs": [],
   "source": [
    "def v(pos,vel,w,action):\n",
    "  global coeff\n",
    "  f=Xi((position,velocity),coeff)\n",
    "  q=np.dot(np.transpose(w[action]),f[action])\n",
    "  return q[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qGdrMScqlJHc"
   },
   "outputs": [],
   "source": [
    "POSITION_MIN = -1.2\n",
    "POSITION_MAX = 0.5\n",
    "VELOCITY_MIN = -0.07\n",
    "VELOCITY_MAX = 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "q7ZKuAZ8lJHd"
   },
   "outputs": [],
   "source": [
    "def convert_position(input_):\n",
    "    output_start=0\n",
    "    output_end=1\n",
    "    input_start=-1.2\n",
    "    input_end=0.5\n",
    "    output = output_start + ((output_end - output_start) / (input_end - input_start)) * (input_ - input_start)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u5iBiCx_lJHd"
   },
   "outputs": [],
   "source": [
    "def convert_velocity(input_):\n",
    "    output_start=0\n",
    "    output_end=1\n",
    "    input_start=-0.07\n",
    "    input_end=0.07\n",
    "    output = output_start + ((output_end - output_start) / (input_end - input_start)) * (input_ - input_start)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sLmrYaiIlJHd"
   },
   "outputs": [],
   "source": [
    "w = np.zeros(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7pFHOQrdlJHe"
   },
   "outputs": [],
   "source": [
    "theta = np.zeros(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "l3rq7j8UlJHf"
   },
   "outputs": [],
   "source": [
    "def policy(pos,vel):\n",
    "    global theta,actions\n",
    "    f=new_Xi(pos,vel)\n",
    "    probs=[]\n",
    "    d=np.exp(np.dot(np.transpose(theta[-1]),f[-1]))+np.exp(np.dot(np.transpose(theta[0]),f[0]))+np.exp(np.dot(np.transpose(theta[1]),f[1]))\n",
    "    for a in actions:\n",
    "        n=np.exp(np.dot(np.transpose(theta[a]),f[a]))\n",
    "        probs.append(n/d)\n",
    "        #print(theta[a])\n",
    "    indices = [i for i, x in enumerate(probs) if x == max(probs)]\n",
    "    t=random.choice(indices)\n",
    "    #print(probs)\n",
    "    #print(t)\n",
    "    #print(actions[t])\n",
    "    return probs,actions[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IOfmrCd9lJHg"
   },
   "outputs": [],
   "source": [
    "l_theta=0.9\n",
    "l_w=0.9\n",
    "alpha_theta= 0.00001\n",
    "alpha_w=0.0056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zmRxSIPIlJHg"
   },
   "outputs": [],
   "source": [
    "class MountainCar:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.actions = [-1, 0 , 1]      # [backward, not_moving, forward]\n",
    "        self.reward = -1.0              #\n",
    "        self.state_lb = [-1.2, -0.07]   # state lower bound\n",
    "        self.state_ub = [0.5, 0.07]     # state upper bound\n",
    "        self.goal_reached = False\n",
    "   \n",
    "    def move(self, position, velocity, action):\n",
    "        \n",
    "        # Update velocity\n",
    "        vp = velocity + 0.001*action - 0.0025*np.cos(3*position)\n",
    "        vp = min(max(vp,self.state_lb[1]),self.state_ub[1])\n",
    "\n",
    "        # update position\n",
    "        xp = position + vp\n",
    "        xp = min(max(xp,self.state_lb[0]),self.state_ub[0])\n",
    "\n",
    "        # if in left bound\n",
    "        if xp == self.state_lb[0]:\n",
    "            vp = 0.0\n",
    "            \n",
    "        # if in right bound\n",
    "        if xp == self.state_ub[0]:\n",
    "            self.goal_reached = True\n",
    "            \n",
    "        return xp, vp, self.reward, self.goal_reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Uw0s02RRlJHg"
   },
   "outputs": [],
   "source": [
    "def delta_lnP(Position,Velocity,action):\n",
    "  f=new_Xi(Position,Velocity)\n",
    "  xa=f[action]\n",
    "  prob,currentAction=policy(Position,Velocity)\n",
    "  term2=0\n",
    "  for i in range(3):\n",
    "      term2=term2+(prob[i]*f[i-1])\n",
    "  return xa-term2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBuUryb-lJHh"
   },
   "source": [
    "## Part I (COMP5500)\n",
    "\n",
    "Implement ACTOR-CRITIC with Eligibility Trace (p. 332).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Unatsf6VlJHh"
   },
   "outputs": [],
   "source": [
    "# Your code here (you are allowed to import from an external python file (of your own impmenetation) \n",
    "# instead of copying all the code here)\n",
    "m=MountainCar()\n",
    "def ActorCritic(episodes,gamma):\n",
    "    global l_theta,l_w,alpha_theta,alpha_w,w,theta\n",
    "    s=[]\n",
    "    r=[]\n",
    "    for k in range(episodes):\n",
    "        steps=0\n",
    "        r_total=0\n",
    "        print(k)\n",
    "        currentPosition = np.random.uniform(-0.6, -0.4)\n",
    "        currentVelocity = 0.0\n",
    "        z_theta=np.zeros((48,1))\n",
    "        z_w=np.zeros((48,1))\n",
    "        \n",
    "        i=1\n",
    "        while currentPosition!=POSITION_MAX:\n",
    "            prob,currentAction=policy(currentPosition,currentVelocity)\n",
    "            #print(prob)\n",
    "            #print(currentAction)\n",
    "            #print(prob)\n",
    "            #print(\"+++++++++++++++++++++++++++\")\n",
    "            newPostion, newVelocity, reward,_ =m.move(currentPosition, currentVelocity, currentAction)\n",
    "            if newPostion==POSITION_MAX:\n",
    "                v_newps=0\n",
    "            else:\n",
    "                v_newps=v(newPostion,newVelocity,w,currentAction)\n",
    "            #print(v_newps)\n",
    "            v_currentpos=v(currentPosition,currentVelocity,w,currentAction)\n",
    "            delta=reward+(gamma*v_newps)-v_currentpos\n",
    "            #print(delta)\n",
    "            f=new_Xi(currentPosition,currentVelocity)\n",
    "            z_w=(gamma*z_w*l_w)+f[currentAction]\n",
    "            z_theta=(gamma*z_theta*l_theta)+(i*delta_lnP(currentPosition,currentVelocity,currentAction))\n",
    "            w[currentAction]+=(alpha_w*delta*z_w)\n",
    "            #print(w[currentAction])\n",
    "            theta[currentAction]+=(alpha_theta*delta*z_theta)\n",
    "            #print(theta[currentAction])\n",
    "            i=i*gamma\n",
    "            currentPosition = newPostion\n",
    "            currentVelocity = newVelocity\n",
    "            steps+=1\n",
    "            r_total=r_total+reward\n",
    "        s.append(steps)\n",
    "        r.append(r_total)\n",
    "    return r,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "Yah_v_vmlJHi",
    "outputId": "8c885c0c-b8ec-4cde-cb3d-5ab989dbe4ad"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5ddebe7307ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-b9a69c73f97c>\u001b[0m in \u001b[0;36mActorCritic\u001b[0;34m(episodes, gamma)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mcurrentPosition\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mPOSITION_MAX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrentAction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentPosition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrentVelocity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;31m#print(prob)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m#print(currentAction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ffd924cea0fb>\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(pos, vel)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print(theta[a])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ffd924cea0fb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print(theta[a])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r,s=ActorCritic(50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmikkLj8_szL"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "plt.plot(np.arange(50),r)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('reward per episode')\n",
    "#plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vdps7fW27LPy"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "plt.plot(np.arange(50),s)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps per episode')\n",
    "#plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwsTef2blJHi"
   },
   "source": [
    "## Part II\n",
    "\n",
    "Use the algorithm to learn the Mountain Car task. Tune the step-size parameter ($\\alpha$), the Function Approximation order, the discount factor ($\\gamma$), and the $\\lambda$-value. **Note:** you can consider the problem to be undiscounted.\n",
    " \n",
    "1. Plot step-per-episode (in log scale) vs. number of episodes. This plot should be averaged over 50-100 runs. \n",
    "2. Plot total reward on episode vs. number of episodes. This plot should be averaged over 50-100 runs.\n",
    "3. Show an animation of the task for the final episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-03Y3UPblJHi"
   },
   "source": [
    "## Part III*\n",
    "\n",
    "Implement the other algorithm and include plots for both algorithms in part II."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW8onehot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
