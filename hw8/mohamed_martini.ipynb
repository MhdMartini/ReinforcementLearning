{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    COMP4600/5500 - Reinforcement Learning\n",
    "\n",
    "# Homework 8 - Policy Gradient\n",
    "\n",
    "### Due: Monday, November 29th 11:59 pm\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Mohamed Martini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to study different properties of Policy Gradient algorithms with Function Approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# You are allowed to use the following modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mountain_car import MountainCar\n",
    "import pygame as pg\n",
    "from itertools import product, count\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\"\"\"\n",
    "Comment if you get an error, or install the following library to use latex with matplotlib on Linux:\n",
    "sudo apt-get install texlive-latex-extra texlive-fonts-recommended dvipng cm-super\n",
    "\"\"\"\n",
    "plt.rcParams['text.usetex'] = True\n"
   ]
  },
  {
   "attachments": {
    "mc.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task description\n",
    "Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram in the upper left of the following figure. The difficulty is that gravity is stronger than the car's engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to first move away from the goal and up the opposite slope on the left. Then by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way.\n",
    "\n",
    "\n",
    "![mc.png](attachment:mc.png)\n",
    "\n",
    "\n",
    "This is a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. The reward in this problem is -1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (-1), and zero throttle (0). The car moves according to a simplified physics. Its position $x_t$ and velocity $\\dot{x}_t$ are updated by\n",
    "\n",
    "$x_{t+1} \\doteq \\text{bound}[x_t + \\dot{x}_{t+1}]$\n",
    "\n",
    "$\\dot{x}_{t+1} \\doteq \\text{bound}[\\dot{x}_t + 0.001 A_t - 0.0025 \\cos(3x_t)]$\n",
    "\n",
    "\n",
    "where the \\textit{bound} operation enforces $-1.2 \\le x_{t+1} \\le 0.5$ and $-0.07 \\le \\dot{x}_{t+1} \\le 0.07$. In addition, when $x_{t+1}$ reached the left bound, $\\dot{x}_{t+1}$ was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode starts from a random position $x_t \\in [-0.6, -0.4)$ and zero velocity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Note:** You have been given a simple implementation of the Mountain Car task. You can use your implementation of the function approximation from Homework 6, or implement a new one. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode():\n",
    "    car = MountainCar()\n",
    "    x = np.random.uniform(-0.6, -0.4)\n",
    "    v = 0\n",
    "    X = [x]\n",
    "    while True:\n",
    "        a = np.random.choice(car.actions)\n",
    "        x, v, r, goal_reached = car.move(x, v, a)\n",
    "        X.append(x)\n",
    "        if goal_reached:\n",
    "            return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 40\n",
    "BG_COLOR = pg.Color(200, 200, 200)\n",
    "CURVE_COLOR = pg.Color(70, 70, 70)\n",
    "CAR_COLOR = pg.Color(200, 70, 70)\n",
    "\n",
    "SCALE = 500\n",
    "PAD = 200\n",
    "\n",
    "X_MIN, X_MAX = -1.2, 0.5\n",
    "Y_MAX, Y_MIN = 1, 0\n",
    "\n",
    "WIDTH = int((X_MAX - X_MIN) * SCALE  + PAD)\n",
    "HEIGHT = int((Y_MAX - Y_MIN) * SCALE  + PAD)\n",
    "\n",
    "\n",
    "def transform(x, y):\n",
    "    \"\"\"transform an xy coordinate to pygame screen coordinates\"\"\"\n",
    "    return (x + 1.2) * SCALE + PAD / 2, (1 - y) * SCALE + PAD / 2\n",
    "\n",
    "def curve(x):\n",
    "    return 0.45 * np.sin(3*x) + 0.55\n",
    "\n",
    "\n",
    "class PgCar:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.screen, self.bg = self.init()\n",
    "        \n",
    "    def init(self):\n",
    "        pg.init()  # initialize pygame\n",
    "        screen = pg.display.set_mode((WIDTH, HEIGHT))  # set up the screen\n",
    "        pg.display.set_caption(\"Mohamed Martini\")  # add a caption\n",
    "        bg = pg.Surface(screen.get_size())  # get a background surface\n",
    "        bg = bg.convert()\n",
    "        bg.fill(BG_COLOR)\n",
    "        screen.blit(bg, (0, 0))\n",
    "        return screen, bg\n",
    "\n",
    "    def draw_curve(self):\n",
    "        start = None\n",
    "        for x in np.arange(X_MIN, X_MAX, 0.001):\n",
    "            end = transform(x, curve(x))\n",
    "            try:\n",
    "                pg.draw.line(self.screen, CURVE_COLOR, start, end, width=7)\n",
    "            except:\n",
    "                continue\n",
    "            finally:\n",
    "                start = end\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"show the grid array on the screen\"\"\"\n",
    "        pg.display.flip()\n",
    "        pg.display.update()\n",
    "    \n",
    "    def reset_screen(self):\n",
    "        self.screen.fill(BG_COLOR)\n",
    "        self.draw_curve()\n",
    "    \n",
    "    def animate(self, X):\n",
    "        \"\"\"receive a list of positions on the x axis, and plot the movement of the screen\"\"\"\n",
    "        clock = pg.time.Clock()\n",
    "        radius = 20\n",
    "        i = 0\n",
    "        num_steps = len(X)\n",
    "        run = True\n",
    "        while run:\n",
    "            clock.tick(FPS)\n",
    "            for event in pg.event.get():\n",
    "                if event.type == pg.QUIT:\n",
    "                    run = False\n",
    "            if i == num_steps - 1:\n",
    "                continue\n",
    "            i += 1\n",
    "            center = np.array((X[i], curve(X[i])))\n",
    "            center = transform(*center)\n",
    "            self.reset_screen()\n",
    "            pg.draw.circle(self.screen, CAR_COLOR, center, radius, width=radius)\n",
    "            self.render()\n",
    "        pg.quit()\n",
    "\n",
    "# X = episode()      \n",
    "# PgCar().animate(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = np.array((1.2, 0.07))\n",
    "scale = np.array((1.7, 0.14))\n",
    "\n",
    "def xv_to_s(xv):\n",
    "    \"\"\"transform from xv to normalized state\"\"\"\n",
    "    return (xv + shift) / scale\n",
    "\n",
    "def s_to_xv(s):\n",
    "    \"\"\"transform from normalized state to xv\"\"\"\n",
    "    return s * scale - shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _x_s(s: np.array):\n",
    "    \"\"\"return x(s) as fourier basis of state\"\"\"\n",
    "    x = np.zeros(NUM_FEATURES)\n",
    "    for i, c in enumerate(product(range(D + 1), repeat=K)):\n",
    "        c = np.array(c)\n",
    "        x[i] = np.cos(np.pi * s.T @ c)\n",
    "    return x\n",
    "\n",
    "\n",
    "def x_s(s: np.array):\n",
    "    \"\"\"\n",
    "    return x(s) as fourier basis of state. \n",
    "    Look first if previously computated, else compute and store it\n",
    "    \"\"\"\n",
    "    global BASIS\n",
    "    try:\n",
    "        return BASIS[tuple(s)]\n",
    "    except KeyError:\n",
    "        BASIS[tuple(s)] = _x_s(s)\n",
    "        return BASIS[tuple(s)]\n",
    "\n",
    "def x_sa(s: np.array, a: int):\n",
    "    \"\"\"return x(s, a) as fourier basis of state, shifted according to the action index\"\"\"\n",
    "    x = np.zeros(NUM_FEATURES * NUM_ACTIONS)\n",
    "    start = NUM_FEATURES * a\n",
    "    end = start + NUM_FEATURES\n",
    "    x[start: end] = x_s(s)\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def h_s(s: np.array, theta: np.array):\n",
    "    \"\"\"return actions' preferences in state s\"\"\"\n",
    "    h = np.zeros(NUM_ACTIONS)\n",
    "    for a in range(NUM_ACTIONS):\n",
    "        h[a] = theta @ x_sa(s, a)\n",
    "    return h\n",
    "\n",
    "def pi_s(s: np.array, theta: np.array):\n",
    "    \"\"\"return policy at state s\"\"\"\n",
    "    h = h_s(s, theta)\n",
    "    exp = np.exp(h - np.max(h))\n",
    "    return exp / np.sum(exp)\n",
    "\n",
    "def v_s(s: np.array, w: np.array):\n",
    "    \"\"\"return the value of a state given the weights vector of a specific action\"\"\"\n",
    "    return w @ x_s(s)\n",
    "\n",
    "def get_action(s, theta):\n",
    "    \"\"\"return index of action at state s according to weights theta\"\"\"\n",
    "    policy = pi_s(s, theta)\n",
    "    return np.random.choice(range(NUM_ACTIONS), p=policy), policy\n",
    "\n",
    "def get_pi_gradient(s, a, policy):\n",
    "    \"\"\"compute gradient ln pi(a|s, theta), which equals x(s,a) = \\sum_b \\pi(b|s, theta) x(s,b)\"\"\"\n",
    "    x = x_sa(s, a)\n",
    "    summation = 0\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        summation += policy[i] * x_sa(s, i)\n",
    "    return x - summation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I (COMP4600)\n",
    "\n",
    "Implement REINFORCE with Baseline (p. 330).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_w_baseline(num_episodes):\n",
    "\n",
    "    gamma = 1\n",
    "    alpha_w = 2e-11\n",
    "    alpha_theta = 2e-11\n",
    "\n",
    "    theta = np.zeros(NUM_ACTIONS * NUM_FEATURES)  # theta for each action\n",
    "    W = np.zeros(NUM_FEATURES)  # weights for estimating v_s\n",
    "\n",
    "    steps_per_e = np.zeros(num_episodes)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # generate an episode following policy\n",
    "        car = MountainCar()\n",
    "\n",
    "        A_t = []\n",
    "        S_t = []\n",
    "        Pi_t = []\n",
    "\n",
    "        xv = np.array((np.random.uniform(-0.6, -0.4), 0))\n",
    "        s = xv_to_s(xv)\n",
    "\n",
    "        for t in count():\n",
    "            # select action\n",
    "            a, policy = get_action(s, theta)\n",
    "            S_t.append(s)\n",
    "            A_t.append(a)\n",
    "            Pi_t.append(policy)\n",
    "\n",
    "            # take action, observe reward and next state\n",
    "            x, v = s_to_xv(s)\n",
    "            xp, vp, r, goal_reached = car.move(x, v, A[a])\n",
    "            sp = xv_to_s(np.array((xp, vp)))\n",
    "            \n",
    "            # calculate the error (delta) - account for terminal state\n",
    "            if goal_reached:\n",
    "                steps_per_e[episode] = t\n",
    "                break\n",
    "            s = sp\n",
    "        \n",
    "        T = t\n",
    "        for t in range(T):\n",
    "            G = -1 * (T - t)\n",
    "            delta = G - v_s(S_t[t], W)\n",
    "            W += alpha_w * delta * x_s(S_t[t])\n",
    "            theta += alpha_theta * delta * get_pi_gradient(S_t[t], A_t[t], Pi_t[t])\n",
    "                \n",
    "    return steps_per_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIS = dict()  # to store the used fourier basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode = 11\n"
     ]
    }
   ],
   "source": [
    "A = np.array([-1, 0, 1])\n",
    "NUM_ACTIONS = 3\n",
    "K = 2\n",
    "D = 3\n",
    "NUM_FEATURES = (D + 1) ** K\n",
    "\n",
    "runs = 50\n",
    "num_episodes = 50\n",
    "\n",
    "steps_per_e_per_run_reinforce = np.zeros((runs, num_episodes))\n",
    "for run in tqdm(range(runs)):\n",
    "    steps_per_e = reinforce_w_baseline(num_episodes)\n",
    "    steps_per_e_per_run_reinforce[run, :] = steps_per_e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I (COMP5500)\n",
    "\n",
    "Implement ACTOR-CRITIC with Eligibility Trace (p. 332).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_et(num_episodes):\n",
    "    car = MountainCar()\n",
    "    gamma = 1\n",
    "    theta = np.zeros(NUM_ACTIONS * NUM_FEATURES)  # theta for each action\n",
    "    W = np.zeros(NUM_FEATURES)  # weights for estimating v_s\n",
    "    \n",
    "    lambda_w = 0.8\n",
    "    lambda_theta = 0.8\n",
    "    \n",
    "    alpha_w = 1e-3\n",
    "    alpha_theta = 1e-3\n",
    "    \n",
    "    steps_per_e = np.zeros(num_episodes)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # initialize s\n",
    "        xv = np.array((np.random.uniform(-0.6, -0.4), 0))\n",
    "        s = xv_to_s(xv)\n",
    "\n",
    "        # reset z vectors\n",
    "        z_theta = np.zeros_like(theta)\n",
    "        z_w = np.zeros_like(W)\n",
    "\n",
    "        # reset gamma multiplier\n",
    "        I = 1\n",
    "        \n",
    "        # reset trajectory\n",
    "        traj = [xv[0]]\n",
    "        \n",
    "        # loop through episode\n",
    "        for t in count():\n",
    "            # select action\n",
    "            a, policy = get_action(s, theta)\n",
    "            \n",
    "            # take action, observe reward and next state\n",
    "            x, v = s_to_xv(s)\n",
    "            xp, vp, r, goal_reached = car.move(x, v, A[a])\n",
    "            sp = xv_to_s(np.array((xp, vp)))\n",
    "            traj.append(xp)\n",
    "            \n",
    "            # calculate the error (delta) - account for terminal state\n",
    "            if goal_reached:\n",
    "                v_sp = 0\n",
    "            else:\n",
    "                v_sp = v_s(sp, W)\n",
    "            delta = r + gamma * v_sp  - v_s(s, W)\n",
    "            \n",
    "            # update z_w\n",
    "            z_w = gamma * lambda_w * z_w + x_s(s)\n",
    "            \n",
    "            # update z_theta\n",
    "            gradient = get_pi_gradient(s, a, policy)\n",
    "            z_theta = gamma * lambda_theta * z_theta + I * gradient\n",
    "            \n",
    "            # update w\n",
    "            W += alpha_w * delta * z_w\n",
    "            \n",
    "            # update theta\n",
    "            theta += alpha_theta * delta * z_theta\n",
    "            \n",
    "            if goal_reached:\n",
    "                car.goal_reached = False\n",
    "                break\n",
    "            \n",
    "            I *= gamma\n",
    "            s = sp\n",
    "        \n",
    "        steps_per_e[episode] = t\n",
    "            \n",
    "    return theta, traj, steps_per_e\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "Use the algorithm to learn the Mountain Car task. Tune the step-size parameter ($\\alpha$), the Function Approximation order, the discount factor ($\\gamma$), and the $\\lambda$-value. **Note:** you can consider the problem to be undiscounted.\n",
    " \n",
    "1. Plot step-per-episode (in log scale) vs. number of episodes. This plot should be averaged over 50-100 runs. \n",
    "2. Plot total reward on episode vs. number of episodes. This plot should be averaged over 50-100 runs.\n",
    "3. Show an animation of the task for the final episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-88f4dd33c492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msteps_per_e_per_run_actor_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_critic_et\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "runs = 50\n",
    "num_episodes = 50\n",
    "\n",
    "steps_per_e_per_run_actor_critic = np.zeros((runs, num_episodes))\n",
    "for run in tqdm(range(runs)):\n",
    "    _, traj, steps_per_e = actor_critic_et(num_episodes)\n",
    "    steps_per_e_per_run_actor_critic[run, :] = np.copy(steps_per_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "fig.suptitle(\"Reinforce with Baseline vs.  ACTOR-CRITIC with Eligibility Trace - Mountain Car Problem\\n \\\n",
    "$\\gamma = 1$. $a_w = a_{\\\\theta} = 0.001$. $\\lambda_w = \\lambda_{\\\\theta} = 0.8$\",\n",
    "            fontsize=24)\n",
    "xlabel = \"Episode Number\"\n",
    "ylabels = [\"Average Number of Timesteps (log scale)\", \"Average Reward\"]\n",
    "\n",
    "axes[0].plot(np.log10(steps_per_e_per_run_reinforce.mean(0)), label=\"Reinforce with Baseline\")\n",
    "axes[0].plot(np.log10(steps_per_e_per_run_actor_critic.mean(0)), label=\"ACTOR-CRITIC with Eligibility Trace\")\n",
    "\n",
    "axes[1].plot(-1 * steps_per_e_per_run_reinforce.mean(0), label=\"Reinforce with Baseline\")  # reward = - number of steps\n",
    "axes[1].plot(-1 * steps_per_e_per_run_actor_critic.mean(0), label=\"ACTOR-CRITIC with Eligibility Trace\")  # reward = - number of steps\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_xlabel(xlabel, fontsize=18)\n",
    "    ax.set_ylabel(ylabels[i], fontsize=18)\n",
    "    ax.set_title(f\"{ylabels[i]} per Episode over {runs} Runs\", fontsize=20)\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_e_per_run_reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = PgCar()\n",
    "car.animate(traj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III*\n",
    "\n",
    "Implement the other algorithm and include plots for both algorithms in part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (you are allowed to import from an external python file (of your own impmenetation) \n",
    "# instead of copying all the code here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
